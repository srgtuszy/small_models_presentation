<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <title>Small Models Running in Your App</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.6.1/dist/reveal.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.6.1/dist/theme/black.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.6.1/plugin/highlight/monokai.css">

    <style>
        .reveal-viewport {
            background: #191919;
        }

        /* Make slides fill viewport and use flexbox for vertical distribution */
        .reveal .slides section,
        .reveal .slides section.present,
        .reveal .slides section.future,
        .reveal .slides section.past,
        .reveal .slides>section,
        .reveal .slides>section>section {
            height: 100% !important;
            top: 0 !important;
            display: flex !important;
            flex-direction: column !important;
            justify-content: flex-start !important;
            padding: 1em 1.2em !important;
            box-sizing: border-box !important;
            gap: 0.3em;
        }

        .reveal .slides section>* {
            margin: 0;
            flex-shrink: 0;
        }

        /* Key content elements grow to fill vertical space */
        .reveal .slides section .cols,
        .reveal .slides section .card,
        .reveal .slides section .demo-box,
        .reveal .slides section pre {
            flex-grow: 1;
            flex-shrink: 1;
        }

        .reveal .slides section .cols {
            align-items: stretch;
        }

        /* Cards inside cols should grow to fill their column */
        .reveal .cols>.card {
            display: flex;
            flex-direction: column;
        }

        .reveal .cols>.card>ul,
        .reveal .cols>.card>p {
            flex-grow: 1;
        }

        /* Prevent speaker notes from being styled as flex content */
        .reveal .slides section aside.notes {
            display: none !important;
        }

        .reveal h1,
        .reveal h2,
        .reveal h3,
        .reveal h4 {
            text-transform: none;
            letter-spacing: -0.02em;
            font-weight: 700;
            margin: 0.2em 0;
        }

        .reveal h1 {
            font-size: 2.2em;
        }

        .reveal h2 {
            font-size: 1.6em;
            margin-bottom: 0.4em;
        }

        .reveal h3 {
            font-size: 1.25em;
        }

        .reveal h4 {
            font-size: 1.1em;
            margin-bottom: 0.15em;
        }

        .reveal section {
            text-align: left;
        }

        .reveal section.center,
        .reveal .slides section.center {
            text-align: center;
            justify-content: center !important;
        }

        .reveal p {
            margin: 0.25em 0;
            font-size: 1em;
        }

        .reveal .lead {
            font-size: 1.2em;
            margin: 0.4em 0;
        }

        .reveal .sub {
            color: #888;
            font-size: 0.8em;
        }

        .reveal table {
            margin: 0.3em 0;
            font-size: 0.8em;
            width: 100%;
        }

        .reveal table th {
            background: rgba(66, 175, 250, 0.25);
            text-transform: none;
        }

        .reveal table td,
        .reveal table th {
            padding: 0.35em 0.6em;
            border: 1px solid rgba(255, 255, 255, 0.1);
        }

        .reveal .hl {
            color: #42affa;
            font-weight: bold;
        }

        .reveal .accent {
            color: #f0a868;
        }

        .reveal .green {
            color: #8be9c0;
        }

        .reveal .dim {
            color: #888;
            font-size: 0.9em;
        }

        .reveal .warn {
            color: #ff6b6b;
        }

        .reveal code:not([class]) {
            background: rgba(0, 0, 0, 0.4);
            padding: 0.15em 0.35em;
            border-radius: 4px;
            font-size: 0.85em;
        }

        .reveal pre {
            width: 100%;
            margin: 0.3em 0;
            flex-grow: 1;
        }

        .reveal pre code {
            padding: 0.7em 0.9em;
            font-size: 0.7em;
            line-height: 1.5;
            max-height: none;
            border-radius: 8px;
        }

        .reveal .cols {
            display: flex;
            gap: 1em;
            align-items: flex-start;
        }

        .reveal .cols>* {
            flex: 1;
        }

        .reveal .card {
            background: rgba(255, 255, 255, 0.05);
            border: 1px solid rgba(255, 255, 255, 0.1);
            border-radius: 8px;
            padding: 0.6em 0.8em;
            margin: 0.25em 0;
            font-size: 0.95em;
        }

        .reveal .card pre code {
            font-size: 0.65em;
        }

        .reveal .big {
            font-size: 2.2em;
            font-weight: 700;
            color: #42affa;
            line-height: 1;
        }

        .reveal .flow {
            display: flex;
            align-items: center;
            gap: 0.5em;
            font-size: 0.85em;
            flex-wrap: wrap;
        }

        .reveal .flow .box {
            background: rgba(66, 175, 250, 0.2);
            border: 1px solid rgba(66, 175, 250, 0.4);
            border-radius: 6px;
            padding: 0.35em 0.6em;
            white-space: nowrap;
        }

        .reveal .flow .arrow {
            color: #42affa;
            font-size: 1.3em;
        }

        .reveal .demo-box {
            background: linear-gradient(135deg, rgba(66, 175, 250, 0.15), rgba(240, 168, 104, 0.15));
            border-radius: 10px;
            padding: 0.8em;
            text-align: center;
            margin: 0.4em 0;
        }

        .reveal ul {
            list-style: none;
            padding: 0;
            margin: 0;
            font-size: 0.95em;
        }

        .reveal ul li {
            position: relative;
            padding-left: 1.1em;
            margin: 0.25em 0;
        }

        .reveal ul li::before {
            content: "▸";
            position: absolute;
            left: 0;
            color: #42affa;
        }

        .reveal ol {
            margin: 0;
            padding-left: 1.4em;
            font-size: 0.95em;
        }

        .reveal ol li {
            margin: 0.25em 0;
        }

        .reveal .speaker-hint {
            position: absolute;
            bottom: 0.5em;
            right: 0.5em;
            font-size: 0.5em;
            color: #444;
            font-style: italic;
        }

        .reveal .example {
            background: rgba(139, 233, 192, 0.1);
            border-left: 4px solid #8be9c0;
            padding: 0.5em 0.7em;
            margin: 0.4em 0;
            font-size: 0.9em;
        }

        .reveal .key-point {
            border-left: 4px solid #42affa;
            padding-left: 0.7em;
            margin: 0.4em 0;
            font-size: 1em;
        }

        .reveal .badge {
            display: inline-block;
            background: rgba(66, 175, 250, 0.3);
            padding: 0.15em 0.5em;
            border-radius: 4px;
            font-size: 0.85em;
            margin-right: 0.3em;
        }

        .reveal .badge.green {
            background: rgba(139, 233, 192, 0.3);
        }

        .reveal .badge.orange {
            background: rgba(240, 168, 104, 0.3);
        }
    </style>
</head>

<body>
    <div class="reveal">
        <div class="slides">

            <!-- ===== SECTION: INTRO ===== -->

            <!-- Title -->
            <section class="center">
                <h1>Small Models Running in Your App</h1>
                <p class="dim">Michał Tuszyński</p>
                <p class="dim" style="font-size: 0.55em; margin-top: 0.3em;">
                    <a href="https://github.com/srgtuszy" target="_blank"
                        style="color: #888; text-decoration: none; margin-right: 1em;">
                        <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor"
                            viewBox="0 0 16 16" style="vertical-align: middle; margin-right: 0.3em;">
                            <path
                                d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.54 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z" />
                        </svg>
                        srgtuszy
                    </a>
                    <a href="https://x.com/srgtuszy" target="_blank" style="color: #888; text-decoration: none;">
                        <svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" fill="currentColor"
                            viewBox="0 0 24 24" style="vertical-align: middle; margin-right: 0.3em;">
                            <path
                                d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z" />
                        </svg>
                        srgtuszy
                    </a>
                </p>
                <p style="margin-top: 1em;"><span class="hl">On-device AI without the cloud</span></p>
                <aside class="notes">
                    <p><strong>Welcome everyone!</strong></p>
                    <p>Today I'll show you how to run AI models directly on mobile devices.</p>
                    <p>No cloud, no API costs, full privacy.</p>
                </aside>
            </section>

            <!-- Agenda -->
            <section>
                <h2>What We'll Cover</h2>
                <div class="cols">
                    <div class="card">
                        <h4>1. Why Small Models</h4>
                        <p>Size vs task matching</p>
                    </div>
                    <div class="card">
                        <h4>2. How They Work</h4>
                        <p>Transformer basics</p>
                    </div>
                </div>
                <div class="cols">
                    <div class="card">
                        <h4>3. Function Calling</h4>
                        <p>The killer mobile feature</p>
                    </div>
                    <div class="card">
                        <h4>4. Two Demos</h4>
                        <p>From scratch to production</p>
                    </div>
                </div>
                <div class="cols">
                    <div class="card">
                        <h4>5. Fine-Tuning</h4>
                        <p>Customize for your app</p>
                    </div>
                    <div class="card">
                        <h4>6. When to Use</h4>
                        <p>Trade-offs & decisions</p>
                    </div>
                </div>
                <aside class="notes">
                    <p>We'll start with WHY small models matter.</p>
                    <p>Then HOW transformers work (simplified).</p>
                    <p>Then FUNCTION CALLING - the key mobile use case.</p>
                    <p>Two working demos you can run.</p>
                    <p>How to fine-tune for your specific functions.</p>
                    <p>When on-device beats cloud.</p>
                </aside>
            </section>

            <!-- ===== SECTION: WHY ===== -->

            <!-- The Problem -->
            <section>
                <h2>The Current Reality</h2>
                <p class="lead">Most apps call <span class="hl">cloud APIs</span> for AI features</p>
                <div class="cols">
                    <div class="card">
                        <h4>Problems with Cloud</h4>
                        <ul>
                            <li>Latency: 500ms - 2 seconds</li>
                            <li>User data leaves the device</li>
                            <li>Per-token pricing adds up</li>
                            <li>Requires internet connection</li>
                        </ul>
                    </div>
                    <div class="card">
                        <h4>Example Scenario</h4>
                        <p class="example">User: "Set reminder for 5pm"<br>
                            → Data sent to OpenAI/Google<br>
                            → Server processes<br>
                            → Response returns<br>
                            → Total: ~1 second, ~$0.001</p>
                    </div>
                </div>
                <aside class="notes">
                    <p>Every time your app calls ChatGPT or Gemini API:</p>
                    <p>1. User waits ~1 second</p>
                    <p>2. Their data goes to someone else's server</p>
                    <p>3. You pay per request</p>
                    <p>4. Doesn't work in airplane mode</p>
                </aside>
            </section>

            <!-- The Insight -->
            <section>
                <h2>The Key Insight</h2>
                <div class="key-point">
                    <p>Most mobile AI tasks <span class="hl">don't need GPT-4 sized models</span></p>
                </div>
                <p class="lead">Think about what your app actually does:</p>
                <table>
                    <tr>
                        <th>Your App Needs</th>
                        <th>Cloud Model?</th>
                        <th>On-Device Works?</th>
                    </tr>
                    <tr>
                        <td>Parse user commands</td>
                        <td class="dim">Overkill</td>
                        <td class="green">✓ 100MB</td>
                    </tr>
                    <tr>
                        <td>Classify text</td>
                        <td class="dim">Overkill</td>
                        <td class="green">✓ 10MB</td>
                    </tr>
                    <tr>
                        <td>Search similar content</td>
                        <td class="dim">Overkill</td>
                        <td class="green">✓ 50MB</td>
                    </tr>
                    <tr>
                        <td>Generate descriptions</td>
                        <td class="dim">Optional</td>
                        <td class="green">✓ 300MB</td>
                    </tr>
                    <tr>
                        <td>Complex research</td>
                        <td class="accent">Yes</td>
                        <td class="dim">No</td>
                    </tr>
                </table>
                <aside class="notes">
                    <p>Ask yourself: Does my app need to write poetry? Analyze scientific papers?</p>
                    <p>Most apps just need to: understand commands, classify content, find similar items.</p>
                    <p>These don't need 1 trillion parameters!</p>
                </aside>
            </section>

            <!-- Size Spectrum -->
            <section>
                <h2>The Model Size Spectrum</h2>
                <div class="cols">
                    <div class="card" style="text-align: center;">
                        <div class="big">1M</div>
                        <p class="dim">Parameters</p>
                        <p>~4MB on disk</p>
                        <p class="green">Runs anywhere</p>
                        <p class="sub">Basic patterns</p>
                    </div>
                    <div class="card" style="text-align: center;">
                        <div class="big">100M</div>
                        <p class="dim">Parameters</p>
                        <p>~400MB on disk</p>
                        <p class="green">Mid-range phones</p>
                        <p class="sub">App commands, VQA</p>
                    </div>
                    <div class="card" style="text-align: center;">
                        <div class="big">1B+</div>
                        <p class="dim">Parameters</p>
                        <p>~4GB on disk</p>
                        <p class="accent">Flagship only</p>
                        <p class="sub">General chat</p>
                    </div>
                </div>
                <div class="key-point">
                    <p><span class="badge">KEY</span> Narrow the task → shrink the model</p>
                </div>
                <aside class="notes">
                    <p>1 million parameters = about 4MB. Fits on ANY phone.</p>
                    <p>100M = about 400MB. Most modern phones can handle this.</p>
                    <p>1 billion+ = need flagship phone with lots of RAM.</p>
                    <p>The smaller the task, the smaller the model you need.</p>
                </aside>
            </section>

            <!-- ===== SECTION: HOW ===== -->

            <!-- What is a Transformer -->
            <section>
                <h2>What Powers These Models?</h2>
                <p class="lead">The <span class="hl">Transformer</span> architecture</p>
                <p class="sub">Don't worry - we'll keep it practical, not academic</p>
                <div class="flow" style="margin: 0.5em 0; font-size: 0.6em;">
                    <span class="box">Input Text</span>
                    <span class="arrow">→</span>
                    <span class="box">Convert to Numbers</span>
                    <span class="arrow">→</span>
                    <span class="box">Process Through Layers</span>
                    <span class="arrow">→</span>
                    <span class="box">Output</span>
                </div>
                <div class="cols">
                    <div class="card">
                        <h4>Good News</h4>
                        <ul>
                            <li>Same architecture as GPT-4</li>
                            <li>Just smaller = fewer layers</li>
                            <li>Core concepts are simple</li>
                        </ul>
                    </div>
                    <div class="card">
                        <h4>Size Comparison</h4>
                        <p>GPT-4: ~100+ layers</p>
                        <p>Tiny model: 2 layers</p>
                        <p class="sub">Same building blocks, different scale</p>
                    </div>
                </div>
                <aside class="notes">
                    <p>Transformers power ChatGPT, Gemini, Claude - all of them.</p>
                    <p>The difference between GPT-4 and a tiny model?</p>
                    <p>GPT-4 has 100+ layers, tiny has 2. Same recipe, smaller batch.</p>
                    <p>You don't need to understand all the math to use them.</p>
                </aside>
            </section>

            <!-- Core Components -->
            <section>
                <h2>Three Core Components</h2>
                <div class="card">
                    <h4>1. Embeddings <span class="badge">Foundation</span></h4>
                    <p>Convert words into number vectors</p>
                    <p class="example">"cat" → [0.23, -0.45, 0.12, ...]</p>
                </div>
                <div class="card">
                    <h4>2. Attention <span class="badge">The Magic</span></h4>
                    <p>Each word "looks at" all other words for context</p>
                </div>
                <div class="card">
                    <h4>3. Feed-Forward <span class="badge">Where Params Live</span></h4>
                    <p>Simple neural network that processes the information</p>
                </div>
                <aside class="notes">
                    <p>Three things you need to know:</p>
                    <p>1. EMBEDDINGS: Words become numbers. Similar words = similar numbers.</p>
                    <p>2. ATTENTION: Words can "see" each other. "Bank" knows if it's near "river" or "money".</p>
                    <p>3. FEED-FORWARD: This is where most parameters live. It's just matrix multiplication.</p>
                </aside>
            </section>

            <!-- Embeddings Detail -->
            <section>
                <h2>Embeddings in Practice</h2>
                <p class="lead">Words become <span class="hl">coordinate points</span> in space</p>
                <pre><code class="language-plaintext">Word        Vector (simplified)
─────────────────────────────────
"cat"       [0.23, -0.45, 0.12]
"feline"    [0.25, -0.42, 0.15]  ← Very similar!
"dog"       [-0.60, 0.30, -0.22] ← Different
"car"       [-0.90, 0.80, 0.50]  ← Very different</code></pre>
                <div class="key-point">
                    <p><span class="badge green">Result</span> Similar meanings → close coordinates</p>
                    <p>This enables: search, similarity, recommendations</p>
                </div>
                <aside class="notes">
                    <p>Think of embeddings as GPS coordinates for words.</p>
                    <p>"Cat" and "feline" are neighbors - same neighborhood.</p>
                    <p>"Cat" and "car" are far apart - different cities.</p>
                    <p>This is why embeddings power search and recommendations.</p>
                </aside>
            </section>

            <!-- Attention Detail -->
            <section>
                <h2>Attention: Understanding Context</h2>
                <div class="example">
                    <p>"I went to the <span class="hl">bank</span> to deposit money"</p>
                    <p>"I went to the <span class="hl">bank</span> to fish"</p>
                </div>
                <p class="lead">Same word "bank" — different meaning</p>
                <div class="card">
                    <h4>How Attention Solves This</h4>
                    <ul>
                        <li>"bank" looks at "deposit money" → financial bank</li>
                        <li>"bank" looks at "fish" → river bank</li>
                        <li>Each word "attends" to relevant context</li>
                    </ul>
                </div>
                <p class="sub">This is why transformers understand language so well</p>
                <aside class="notes">
                    <p>ATTENTION is the breakthrough that made modern AI possible.</p>
                    <p>The word "bank" doesn't know what it means by itself.</p>
                    <p>But it can "look at" other words in the sentence.</p>
                    <p>Near "deposit money"? It's a financial bank.</p>
                    <p>Near "fish"? It's a river bank.</p>
                </aside>
            </section>

            <!-- Efficiency -->
            <section>
                <h2>Making Models Smaller</h2>
                <p class="lead">Three techniques to shrink models</p>
                <div class="cols">
                    <div class="card">
                        <h4>Quantization</h4>
                        <p>Store numbers in less space</p>
                        <p><code>FP32 → INT4</code></p>
                        <p class="green">4× smaller</p>
                        <p class="sub">Minimal quality loss</p>
                    </div>
                    <div class="card">
                        <h4>Weight Tying</h4>
                        <p>Reuse same weights</p>
                        <p>Input = Output embeddings</p>
                        <p class="green">2× smaller</p>
                        <p class="sub">No quality loss</p>
                    </div>
                </div>
                <div class="card">
                    <h4>Pruning</h4>
                    <p>Remove connections that don't matter much</p>
                    <p class="green">Up to 90% smaller</p>
                    <p class="sub">Like trimming a tree</p>
                </div>
                <aside class="notes">
                    <p>QUANTIZATION: Normal models use 32-bit numbers. We can use 4-bit.</p>
                    <p>That's 8× fewer bits per number. Makes models 4× smaller with minimal impact.</p>
                    <p>WEIGHT TYING: The input and output use similar math. Share the weights.</p>
                    <p>PRUNING: Neural networks have many connections. Many are almost zero. Remove them.</p>
                </aside>
            </section>

            <!-- ===== SECTION: FUNCTION CALLING ===== -->

            <!-- Function Calling Intro -->
            <section>
                <h2>Function Calling: The Killer Feature</h2>
                <p class="lead">The #1 use case for mobile AI</p>
                <div class="example">
                    <p>User: <span class="hl">"Set a reminder for tomorrow at 5pm"</span></p>
                    <p>Model: <span class="green">set_reminder(title="...", time="tomorrow 5pm")</span></p>
                </div>
                <div class="cols">
                    <div class="card">
                        <h4>The Problem It Solves</h4>
                        <ul>
                            <li>Natural language → API calls</li>
                            <li>No more regex parsing hell</li>
                            <li>"remind me", "set alarm", "alert" → same function</li>
                        </ul>
                    </div>
                    <div class="card">
                        <h4>Why It Matters</h4>
                        <ul>
                            <li>Voice assistants need this</li>
                            <li>Chatbots need this</li>
                            <li>Every app needs this</li>
                        </ul>
                    </div>
                </div>
                <aside class="notes">
                    <p>Function calling is THE killer feature for mobile AI.</p>
                    <p>Instead of writing parsers for every variation of "set alarm"...</p>
                    <p>"remind me at 5", "alert me tomorrow", "wake me up at 7"...</p>
                    <p>Just let the model figure it out. One function call covers all.</p>
                </aside>
            </section>

            <!-- How Function Calling Works -->
            <section>
                <h2>How Function Calling Works</h2>
                <div class="flow" style="font-size: 0.55em; margin: 0.5em 0;">
                    <span class="box">"Remind me to call mom"</span>
                    <span class="arrow">→</span>
                    <span class="box">Tokenizer</span>
                    <span class="arrow">→</span>
                    <span class="box">Small LLM</span>
                    <span class="arrow">→</span>
                    <span class="box" style="background: rgba(139, 233, 192, 0.2);">set_reminder{...}</span>
                </div>
                <div class="cols">
                    <div class="card">
                        <h4>The Model's Job</h4>
                        <ul>
                            <li>Understand user intent</li>
                            <li>Extract relevant parameters</li>
                            <li>Output structured JSON</li>
                            <li>Match to available functions</li>
                        </ul>
                    </div>
                    <div class="card">
                        <h4>Your App's Job</h4>
                        <ul>
                            <li>Define available functions</li>
                            <li>Execute the function call</li>
                            <li>Show result to user</li>
                        </ul>
                    </div>
                </div>
                <aside class="notes">
                    <p>The model doesn't actually DO anything - it outputs structured data.</p>
                    <p>Your app receives: {"function": "set_reminder", "args": {"time": "5pm"}}</p>
                    <p>Then your app calls the actual set_reminder() function.</p>
                    <p>Model = language understanding. App = actual execution.</p>
                </aside>
            </section>

            <!-- Function Calling Example -->
            <section>
                <h2>Function Calling Example</h2>
                <div class="cols">
                    <div class="card">
                        <h4>App Defines Functions</h4>
                        <pre><code class="language-json">{
  "functions": [
    {"name": "show_alert", 
     "params": ["message"]},
    {"name": "navigate", 
     "params": ["screen"]},
    {"name": "toggle", 
     "params": ["setting"]}
  ]
}</code></pre>
                    </div>
                    <div class="card">
                        <h4>Model Output</h4>
                        <div class="example">
                            <p><strong>User:</strong> "alert hello world"</p>
                            <p><strong>Model:</strong></p>
                            <pre><code class="language-json">{
  "call": "show_alert",
  "msg": "hello world"
}</code></pre>
                        </div>
                    </div>
                </div>
                <div class="key-point">
                    <p>Model maps <span class="hl">natural language</span> → <span class="green">structured
                            output</span></p>
                </div>
                <aside class="notes">
                    <p>1. Your app tells the model what functions exist.</p>
                    <p>2. User speaks/types naturally.</p>
                    <p>3. Model outputs JSON that matches one of your functions.</p>
                    <p>4. Your app executes it.</p>
                </aside>
            </section>

            <!-- Function Calling Models -->
            <section>
                <h2>Models for Function Calling</h2>
                <table>
                    <tr>
                        <th>Model</th>
                        <th>Size</th>
                        <th>Accuracy</th>
                        <th>Best For</th>
                    </tr>
                    <tr>
                        <td>Tiny Transformer</td>
                        <td class="green">~0.8M params</td>
                        <td class="dim">70%</td>
                        <td>Learning/Demo</td>
                    </tr>
                    <tr>
                        <td>FunctionGemma</td>
                        <td class="green">270M</td>
                        <td class="accent">85%</td>
                        <td>Production Apps</td>
                    </tr>
                    <tr>
                        <td>GPT-4o-mini</td>
                        <td class="dim">Cloud</td>
                        <td class="green">95%</td>
                        <td>Complex reasoning</td>
                    </tr>
                </table>
                <div class="cols">
                    <div class="card">
                        <h4>On-Device Advantage</h4>
                        <ul>
                            <li>85% is enough for most apps</li>
                            <li>No latency, no cost</li>
                            <li>Works offline</li>
                        </ul>
                    </div>
                    <div class="card">
                        <h4>When to Use Cloud</h4>
                        <ul>
                            <li>Complex multi-step reasoning</li>
                            <li>Need 95%+ accuracy</li>
                            <li>Can tolerate latency</li>
                        </ul>
                    </div>
                </div>
                <aside class="notes">
                    <p>Tiny Transformer: demo purposes, shows it's possible.</p>
                    <p>FunctionGemma: real production use, fine-tuned for this task.</p>
                    <p>GPT-4o-mini: best accuracy but costs money and needs internet.</p>
                    <p>For most apps, 85% on-device beats 95% cloud due to UX.</p>
                </aside>
            </section>

            <!-- ===== SECTION: DEMO 1 ===== -->

            <!-- Demo 1 Intro -->
            <section>
                <h2>Demo 1: Build From Scratch</h2>
                <div class="demo-box">
                    <div class="big">~0.8M</div>
                    <p class="dim">809K parameters • ~3MB</p>
                    <p>Train in <span class="hl">2-3 minutes</span> on laptop</p>
                </div>
                <div class="cols">
                    <div>
                        <h4>The Goal</h4>
                        <ul>
                            <li>Show it's just code, not magic</li>
                            <li>Same architecture as GPT</li>
                            <li>~420 lines of Python</li>
                        </ul>
                    </div>
                    <div>
                        <h4>Architecture</h4>
                        <ul>
                            <li>4 transformer layers</li>
                            <li>4 attention heads</li>
                            <li>128 embedding dimension</li>
                        </ul>
                    </div>
                </div>
                <aside class="notes">
                    <p>First demo: I'll show you how to BUILD a tiny transformer.</p>
                    <p>Why? To demystify it. It's not magic, it's math and code.</p>
                    <p>This model has ~800,000 parameters. GPT-4 has trillions.</p>
                    <p>But it uses the EXACT same architecture.</p>
                    <p>Trains in minutes, not months. Let's look at the code.</p>
                </aside>
            </section>

            <!-- Demo 1 Code -->
            <section>
                <h2>Tiny Transformer Code</h2>
                <p class="sub">The core is surprisingly simple</p>
                <pre><code class="language-python">class TinyTransformer(nn.Module):
    def __init__(self):
        self.tok_emb = nn.Embedding(vocab_size, 128)
        self.pos_emb = nn.Embedding(80, 128)
        self.blocks = nn.Sequential(
            *[Block() for _ in range(4)]  # 4 layers
        )
        self.head = nn.Linear(128, vocab_size)
        self.head.weight = self.tok_emb.weight  # Weight tying

    def forward(self, x):
        tok = self.tok_emb(x)
        pos = self.pos_emb(torch.arange(len(x)))
        x = self.blocks(tok + pos)
        return self.head(x)</code></pre>
                <p class="key-point"><span class="badge">That's it!</span> The rest is just training loops</p>
                <aside class="notes">
                    <p>This is the whole model definition. About 10 lines.</p>
                    <p>tok_emb: converts tokens to vectors</p>
                    <p>pos_emb: adds position information</p>
                    <p>blocks: the transformer layers</p>
                    <p>head: converts back to vocabulary</p>
                    <p>The full training script is in demo1 folder.</p>
                </aside>
            </section>

            <!-- ===== SECTION: DEMO 2 ===== -->

            <!-- Demo 2 Intro -->
            <section>
                <h2>Demo 2: FunctionGemma 270M</h2>
                <div class="demo-box">
                    <div class="big">270M</div>
                    <p class="dim">Parameters • ~230MB (INT4 quantized)</p>
                    <p>Runs on <span class="hl">mid-range phones</span>, ~500MB RAM</p>
                </div>
                <div class="cols">
                    <div class="card">
                        <h4>What We Built</h4>
                        <ul>
                            <li>llama.cpp native integration</li>
                            <li>Android + iOS support</li>
                            <li>Single function: show_alert</li>
                            <li>Detects alert requests automatically</li>
                        </ul>
                    </div>
                    <div class="card">
                        <h4>Performance</h4>
                        <ul>
                            <li>~0.3s time to first token</li>
                            <li>~126 tokens/second</li>
                            <li>Works offline</li>
                            <li>Zero API cost</li>
                        </ul>
                    </div>
                </div>
                <aside class="notes">
                    <p>This is a real working app using llama.cpp.</p>
                    <p>Model: FunctionGemma 270M, quantized to 4-bit (230MB).</p>
                    <p>Runs on mid-range Android and iPhone 12+.</p>
                    <p>Inference happens entirely on device - no network needed.</p>
                </aside>
            </section>

            <!-- Demo 2 Architecture -->
            <section>
                <h2>How It's Built</h2>
                <div class="flow" style="font-size: 0.55em; margin: 0.5em 0;">
                    <span class="box">Kotlin UI</span>
                    <span class="arrow">→</span>
                    <span class="box">JNI Bridge</span>
                    <span class="arrow">→</span>
                    <span class="box">llama.cpp</span>
                    <span class="arrow">→</span>
                    <span class="box" style="background: rgba(139, 233, 192, 0.2);">GGUF Model</span>
                </div>
                <div class="card">
                    <h4>Architecture: llama.cpp Native</h4>
                    <pre><code class="language-plaintext">┌─────────────────┐
│  Compose UI     │  ← Cross-platform UI
├─────────────────┤
│  LLM Engine     │  ← Platform interface
├─────────────────┤
│  llama.cpp      │  ← C++ inference engine
├─────────────────┤
│  GGUF Model     │  ← Quantized weights (230MB)
└─────────────────┘</code></pre>
                </div>
                <aside class="notes">
                    <p>We use llama.cpp - the most efficient CPU inference engine.</p>
                    <p>JNI bridges Kotlin to C++ on Android.</p>
                    <p>GGUF format with INT4 quantization shrinks model 4x.</p>
                    <p>All runs on CPU - no GPU or NPU needed.</p>
                </aside>
            </section>

            <!-- ===== SECTION: FINE-TUNING ===== -->

            <!-- Fine-Tuning Intro -->
            <section>
                <h2>Fine-Tuning FunctionGemma</h2>
                <p class="lead">Customize for <span class="hl">your specific tools</span></p>
                <div class="cols">
                    <div class="card">
                        <h4>Why Fine-Tune?</h4>
                        <ul>
                            <li>Learn your custom functions</li>
                            <li>Improve accuracy from 58% → 85%+</li>
                            <li>Match your app's terminology</li>
                            <li>Reduce refusals</li>
                        </ul>
                    </div>
                    <div class="card">
                        <h4>What You Need</h4>
                        <ul>
                            <li>Training data (100-1000 examples)</li>
                            <li>GPU (Colab free tier works!)</li>
                            <li>1-2 hours training time</li>
                            <li>~10MB VRAM for 270M model</li>
                        </ul>
                    </div>
                </div>
                <aside class="notes">
                    <p>FunctionGemma is designed to be fine-tuned - it's not a general chatbot.</p>
                    <p>Train it on YOUR functions, YOUR terminology, YOUR use cases.</p>
                    <p>The base model gets 58% on benchmarks. Fine-tuned: 85%+.</p>
                    <p>270M parameters is small enough to train on free Google Colab!</p>
                </aside>
            </section>

            <!-- Fine-Tuning Tools -->
            <section>
                <h2>Fine-Tuning Tools</h2>
                <div class="cols">
                    <div class="card">
                        <h4>Unsloth <span class="badge green">Recommended</span></h4>
                        <ul>
                            <li>30x faster than standard</li>
                            <li>90% less VRAM usage</li>
                            <li>Works on free Colab</li>
                            <li>Auto GGUF export</li>
                        </ul>
                    </div>
                    <div class="card">
                        <h4>Hugging Face TRL</h4>
                        <ul>
                            <li>Standard approach</li>
                            <li>More configurable</li>
                            <li>Slower training</li>
                            <li>Needs more VRAM</li>
                        </ul>
                    </div>
                </div>
                <div class="key-point">
                    <p><span class="badge orange">Note</span> Unsloth requires a GPU (NVIDIA, AMD, or Intel). Mac users:
                        use Google Colab
                        (free T4 GPU)</p>
                </div>
                <aside class="notes">
                    <p>Unsloth is the fastest way to fine-tune LLMs.</p>
                    <p>They hand-write GPU kernels for 30x speedup.</p>
                    <p>Works on NVIDIA, AMD, and Intel GPUs. Not Apple Silicon yet.</p>
                    <p>For Mac: use Google Colab's free GPU tier.</p>
                </aside>
            </section>

            <!-- Fine-Tuning Process -->
            <section>
                <h2>Fine-Tuning Process</h2>
                <div class="flow" style="font-size: 0.55em; margin: 0.5em 0;">
                    <span class="box">Training Data</span>
                    <span class="arrow">→</span>
                    <span class="box">LoRA Config</span>
                    <span class="arrow">→</span>
                    <span class="box">Train 1-2 hrs</span>
                    <span class="arrow">→</span>
                    <span class="box">Export GGUF</span>
                    <span class="arrow">→</span>
                    <span class="box" style="background: rgba(139, 233, 192, 0.2);">Mobile Ready!</span>
                </div>
                <div class="card">
                    <h4>Training Data Format</h4>
                    <pre><code class="language-json">{
  "conversations": [
    {"role": "user", "content": "Show alert hello"},
    {"role": "assistant", "content": "{\"call\":\"show_alert\",\"msg\":\"hello\"}"}
  ]
}</code></pre>
                </div>
                <div class="cols">
                    <div class="card">
                        <h4>LoRA Benefits</h4>
                        <ul>
                            <li>Train only 1-5% of weights</li>
                            <li>10x smaller checkpoints</li>
                            <li>Can swap adapters</li>
                        </ul>
                    </div>
                    <div class="card">
                        <h4>Hardware Needed</h4>
                        <p>Free Colab T4: ✓ Works</p>
                        <p>RX 6600 8GB: ✓ Works</p>
                        <p>RTX 3060 12GB: ✓ Fast</p>
                    </div>
                </div>
                <aside class="notes">
                    <p>Training data is just input-output pairs.</p>
                    <p>LoRA (Low-Rank Adaptation) lets you train huge models with tiny resources.</p>
                    <p>Instead of updating all 270M parameters, you train ~1M adapters.</p>
                    <p>A free Google Colab T4 GPU can train this in 1-2 hours.</p>
                </aside>
            </section>

            <!-- Colab Notebook -->
            <section>
                <h2>Try It Now: Colab Notebook</h2>
                <div class="demo-box">
                    <p class="big" style="font-size: 1.2em;">1-Click Fine-Tune</p>
                    <p style="font-size: 0.7em;">Open in Google Colab → Run all cells → Download GGUF</p>
                </div>
                <pre><code class="language-python"># Install Unsloth
pip install "unsloth[colab-new]" 

# Load FunctionGemma
from unsloth import FastLanguageModel
model, tokenizer = FastLanguageModel.from_pretrained(
    "unsloth/functiongemma-270m-it"
)

# Configure LoRA
model = FastLanguageModel.get_peft_model(
    model,
    r=16,           # LoRA rank
    target_modules=["q_proj", "k_proj", "v_proj"],
)

# Train with your data
from trl import SFTTrainer
trainer = SFTTrainer(model, train_dataset=your_data)
trainer.train()

# Export for mobile
model.save_pretrained_gguf("model", tokenizer)</code></pre>
                <aside class="notes">
                    <p>This is all you need - about 10 lines of code.</p>
                    <p>Unsloth handles tokenization, chat templates, everything.</p>
                    <p>The Colab notebook has all this ready to run.</p>
                    <p>Export directly to GGUF format for llama.cpp/mobile use.</p>
                </aside>
            </section>

            <!-- Fine-Tuning Results -->
            <section>
                <h2>Results: Before vs After</h2>
                <div class="cols">
                    <div class="card" style="border-color: rgba(255,107,107,0.5);">
                        <h4 class="warn">Before Fine-Tuning</h4>
                        <div class="example">
                            <p><strong>User:</strong> "Show alert hello"</p>
                            <p><strong>Model:</strong> "I cannot directly call functions..."</p>
                        </div>
                        <p class="dim">Base model doesn't know your tools</p>
                    </div>
                    <div class="card" style="border-color: rgba(139,233,192,0.5);">
                        <h4 class="green">After Fine-Tuning</h4>
                        <div class="example" style="background: rgba(139,233,192,0.1);">
                            <p><strong>User:</strong> "Show alert hello"</p>
                            <p><strong>Model:</strong> <span class="green">{"call":"show_alert","msg":"hello"}</span>
                            </p>
                        </div>
                        <p class="green">Correct function call!</p>
                    </div>
                </div>
                <div class="key-point">
                    <p>Just <span class="hl">1 hour</span> of fine-tuning transforms the model</p>
                </div>
                <aside class="notes">
                    <p>Before: the model knows it should call functions, but not YOUR functions.</p>
                    <p>After: it knows exactly what to call and how.</p>
                    <p>This is the difference between 58% and 85%+ accuracy.</p>
                    <p>And it only takes 1-2 hours on free Colab.</p>
                </aside>
            </section>

            <!-- ===== SECTION: DEPLOYMENT ===== -->

            <!-- Mobile Requirements -->
            <section>
                <h2>Running on Mobile</h2>
                <table>
                    <tr>
                        <th>Model Size</th>
                        <th>RAM Needed</th>
                        <th>Works On</th>
                    </tr>
                    <tr>
                        <td>10MB</td>
                        <td>100MB</td>
                        <td class="green">All phones</td>
                    </tr>
                    <tr>
                        <td>100MB</td>
                        <td>500MB</td>
                        <td class="green">Mid-range+</td>
                    </tr>
                    <tr>
                        <td>300MB</td>
                        <td>1GB</td>
                        <td class="accent">Flagships</td>
                    </tr>
                    <tr>
                        <td>1GB+</td>
                        <td>2GB+</td>
                        <td class="dim">High-end only</td>
                    </tr>
                </table>
                <div class="cols">
                    <div class="card">
                        <h4>Rule of Thumb</h4>
                        <p>Model size × 3 = RAM needed</p>
                        <p class="sub">100MB model → ~300MB RAM minimum</p>
                    </div>
                    <div class="card">
                        <h4>Battery Impact</h4>
                        <p>~0.03% per inference</p>
                        <p class="sub">Negligible for typical usage</p>
                    </div>
                </div>
                <aside class="notes">
                    <p>Key insight: RAM is the limiting factor, not storage.</p>
                    <p>Rule of thumb: model size × 3 for RAM.</p>
                    <p>Why? Model weights + activations + overhead.</p>
                    <p>100MB model is safe for most modern phones.</p>
                    <p>Battery impact is minimal - less than video playback.</p>
                </aside>
            </section>

            <!-- Deployment Options -->
            <section>
                <h2>How to Deploy</h2>
                <div class="cols">
                    <div class="card">
                        <h4>MediaPipe <span class="badge green">Recommended</span></h4>
                        <p>Google's official SDK</p>
                        <p>Android, iOS, Web</p>
                        <p>Built-in model support</p>
                    </div>
                    <div class="card">
                        <h4>LiteRT / TFLite</h4>
                        <p>TensorFlow Lite runtime</p>
                        <p>Cross-platform</p>
                        <p>Widely supported</p>
                    </div>
                </div>
                <div class="cols">
                    <div class="card">
                        <h4>llama.cpp</h4>
                        <p>Efficient CPU inference</p>
                        <p>Good for quantized models</p>
                    </div>
                    <div class="card">
                        <h4>ONNX Runtime</h4>
                        <p>Cross-platform</p>
                        <p>Hardware acceleration</p>
                    </div>
                </div>
                <aside class="notes">
                    <p>MediaPipe is Google's solution - easiest for Android/iOS.</p>
                    <p>Supports Gemma, Phi, and other models out of the box.</p>
                    <p>TFLite/LiteRT is more flexible, requires more setup.</p>
                    <p>llama.cpp is great for CPU-only, quantized models.</p>
                    <p>ONNX works well with hardware acceleration (GPU/NPU).</p>
                </aside>
            </section>

            <!-- ===== SECTION: COMPARISON ===== -->

            <!-- Cloud vs On-Device -->
            <section>
                <h2>Cloud vs On-Device</h2>
                <table>
                    <tr>
                        <th>Factor</th>
                        <th>On-Device</th>
                        <th>Cloud API</th>
                    </tr>
                    <tr>
                        <td>Latency</td>
                        <td class="green">~300ms</td>
                        <td class="dim">500ms-2s</td>
                    </tr>
                    <tr>
                        <td>Privacy</td>
                        <td class="green">100% local</td>
                        <td class="dim">Sent to server</td>
                    </tr>
                    <tr>
                        <td>Offline</td>
                        <td class="green">✓ Works</td>
                        <td class="dim">✗ No</td>
                    </tr>
                    <tr>
                        <td>Cost</td>
                        <td class="green">Free</td>
                        <td class="dim">$0.001-0.01/request</td>
                    </tr>
                    <tr>
                        <td>Quality</td>
                        <td class="accent">Good (85%)</td>
                        <td class="dim">Best (95%)</td>
                    </tr>
                    <tr>
                        <td>Model Size</td>
                        <td class="accent">Limited</td>
                        <td class="dim">Unlimited</td>
                    </tr>
                </table>
                <aside class="notes">
                    <p>Let's compare the trade-offs honestly.</p>
                    <p>On-device wins on: latency, privacy, offline, cost.</p>
                    <p>Cloud wins on: quality, model flexibility.</p>
                    <p>For most mobile apps, 85% accuracy is plenty.</p>
                    <p>And you get instant response, no API costs, full privacy.</p>
                </aside>
            </section>

            <!-- When to Use Each -->
            <section>
                <h2>When to Use What</h2>
                <div class="card">
                    <h4 class="green">Use On-Device When:</h4>
                    <ul>
                        <li>Task is well-defined (commands, classification, search)</li>
                        <li>Privacy is important (health, finance apps)</li>
                        <li>Offline support matters</li>
                        <li>Cost sensitivity (many requests)</li>
                        <li>Low latency required (<500ms)< /li>
                    </ul>
                </div>
                <div class="card">
                    <h4 class="accent">Use Cloud When:</h4>
                    <ul>
                        <li>Need best possible quality</li>
                        <li>Complex reasoning required</li>
                        <li>Large context needed (long documents)</li>
                        <li>Request volume is low</li>
                    </ul>
                </div>
                <aside class="notes">
                    <p>Use on-device for: voice commands, text classification, semantic search, recommendations.</p>
                    <p>Use cloud for: complex analysis, research assistance, very long context.</p>
                    <p>You can also HYBRID: on-device for simple stuff, cloud for complex.</p>
                </aside>
            </section>

            <!-- ===== SECTION: SUMMARY ===== -->

            <!-- Key Takeaways -->
            <section class="center">
                <h2>Key Takeaways</h2>
                <div class="cols">
                    <div class="card">
                        <div class="big">1</div>
                        <p>You can <span class="hl">build this</span></p>
                        <p class="sub">Just Python + PyTorch</p>
                    </div>
                    <div class="card">
                        <div class="big">2</div>
                        <p>Size <span class="hl">matters</span></p>
                        <p class="sub">Match model to task</p>
                    </div>
                    <div class="card">
                        <div class="big">3</div>
                        <p>Privacy <span class="hl">wins</span></p>
                        <p class="sub">Data stays on device</p>
                    </div>
                </div>
                <div class="cols">
                    <div class="card">
                        <div class="big">4</div>
                        <p>Works <span class="hl">offline</span></p>
                        <p class="sub">No internet needed</p>
                    </div>
                    <div class="card">
                        <div class="big">5</div>
                        <p>Zero <span class="hl">cost</span></p>
                        <p class="sub">No API fees</p>
                    </div>
                    <div class="card">
                        <div class="big">6</div>
                        <p>Better <span class="hl">UX</span></p>
                        <p class="sub">&lt;300ms response</p>
                    </div>
                </div>
                <aside class="notes">
                    <p>Let me summarize:</p>
                    <p>1. This isn't magic - you can understand and build it.</p>
                    <p>2. Don't over-engineer - small models for focused tasks.</p>
                    <p>3. Your users' data never leaves their phone.</p>
                    <p>4. Works in airplane mode.</p>
                    <p>5. No surprise API bills.</p>
                    <p>6. Instant response = better user experience.</p>
                </aside>
            </section>

            <!-- Resources -->
            <section>
                <h2>Resources</h2>
                <div class="cols">
                    <div class="card">
                        <h4>Learn More</h4>
                        <ul>
                            <li>NanoGPT (Karpathy)</li>
                            <li>Google Gemma docs</li>
                            <li>Attention Is All You Need (paper)</li>
                        </ul>
                    </div>
                    <div class="card">
                        <h4>Fine-Tune FunctionGemma</h4>
                        <ul>
                            <li>Unsloth Colab Notebook</li>
                            <li>Free T4 GPU on Colab</li>
                            <li>1-click GGUF export</li>
                        </ul>
                    </div>
                </div>
                <div class="cols">
                    <div class="card">
                        <h4>Start Building</h4>
                        <ul>
                            <li>MediaPipe LLM Inference</li>
                            <li>llama.cpp for mobile</li>
                            <li>TensorFlow Lite</li>
                        </ul>
                    </div>
                    <div class="card">
                        <h4>Today's Demos</h4>
                        <code>demo1_tiny_transformer/</code><br>
                        <code>demo2_function_gemma/</code>
                    </div>
                </div>
                <aside class="notes">
                    <p>NanoGPT is the best way to learn - Andrej Karpathy's minimal GPT.</p>
                    <p>Gemma models are free and well-documented.</p>
                    <p>Unsloth Colab notebook lets you fine-tune in 1-2 hours for free.</p>
                    <p>MediaPipe is the easiest way to start on Android/iOS.</p>
                </aside>
            </section>

            <!-- Q&A -->
            <section class="center">
                <h1>Questions?</h1>
                <p class="dim" style="margin-top: 0.5em;">Thank you!</p>
                <div style="margin-top: 1em;">
                    <img src="https://api.qrserver.com/v1/create-qr-code/?size=200x200&data=https://github.com/srgtuszy/small_models_presentation&bgcolor=191919&color=ffffff"
                        alt="QR Code to GitHub repo"
                        style="width: 200px; height: 200px; border-radius: 8px; image-rendering: pixelated;" />
                </div>
                <p style="margin-top: 0.5em; font-size: 0.55em;">
                    <a href="https://github.com/srgtuszy/small_models_presentation" target="_blank"
                        style="color: #42affa; text-decoration: none;">
                        github.com/srgtuszy/small_models_presentation
                    </a>
                </p>
                <p class="dim" style="font-size: 0.45em; margin-top: 0.3em;">
                    <a href="https://github.com/srgtuszy" target="_blank"
                        style="color: #888; text-decoration: none; margin-right: 1em;">
                        <svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" fill="currentColor"
                            viewBox="0 0 16 16" style="vertical-align: middle; margin-right: 0.2em;">
                            <path
                                d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.54 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z" />
                        </svg>
                        srgtuszy
                    </a>
                    <a href="https://x.com/srgtuszy" target="_blank" style="color: #888; text-decoration: none;">
                        <svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor"
                            viewBox="0 0 24 24" style="vertical-align: middle; margin-right: 0.2em;">
                            <path
                                d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z" />
                        </svg>
                        srgtuszy
                    </a>
                </p>
                <aside class="notes">
                    <p>Open for questions.</p>
                    <p>Happy to discuss specific use cases or implementation details.</p>
                    <p>Scan the QR code or visit the GitHub link to get all demos and slides.</p>
                </aside>
            </section>

        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.6.1/dist/reveal.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.6.1/plugin/highlight/highlight.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.6.1/plugin/notes/notes.js"></script>
    <script>
        Reveal.initialize({
            hash: true,
            slideNumber: 'c/t',
            transition: 'slide',
            embedded: false,
            center: false,
            width: 1920,
            height: 1080,
            margin: 0.04,
            minScale: 0.2,
            maxScale: 2.0,
            plugins: [RevealHighlight, RevealNotes]
        }).then(() => {
            // Force flex layout on all slides after Reveal.js has been fully initialized
            document.querySelectorAll('.reveal .slides section').forEach(section => {
                section.style.display = 'flex';
                section.style.flexDirection = 'column';
                section.style.justifyContent = section.classList.contains('center') ? 'center' : 'flex-start';
                section.style.height = '100%';
                section.style.top = '0';
            });
        });
    </script>
</body>

</html>